{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import learning_curve\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '../../Datasets/SICK/SICK.txt'\n",
    "data = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>entailment_label</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_AB</th>\n",
       "      <th>entailment_BA</th>\n",
       "      <th>sentence_A_original</th>\n",
       "      <th>sentence_B_original</th>\n",
       "      <th>sentence_A_dataset</th>\n",
       "      <th>sentence_B_dataset</th>\n",
       "      <th>SemEval_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.7</td>\n",
       "      <td>A_entails_B</td>\n",
       "      <td>B_entails_A</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>3.6</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_contradicts_A</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.4</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  A group of kids is playing in a yard and an ol...   \n",
       "1        2  A group of children is playing in the house an...   \n",
       "2        3  The young boys are playing outdoors and the ma...   \n",
       "3        4  The young boys are playing outdoors and the ma...   \n",
       "4        5  The kids are playing outdoors near a man with ...   \n",
       "\n",
       "                                          sentence_B entailment_label  \\\n",
       "0  A group of boys in a yard is playing and a man...          NEUTRAL   \n",
       "1  A group of kids is playing in a yard and an ol...          NEUTRAL   \n",
       "2  The kids are playing outdoors near a man with ...       ENTAILMENT   \n",
       "3  There is no boy playing outdoors and there is ...    CONTRADICTION   \n",
       "4  A group of kids is playing in a yard and an ol...          NEUTRAL   \n",
       "\n",
       "   relatedness_score    entailment_AB    entailment_BA  \\\n",
       "0                4.5      A_neutral_B      B_neutral_A   \n",
       "1                3.2  A_contradicts_B      B_neutral_A   \n",
       "2                4.7      A_entails_B      B_entails_A   \n",
       "3                3.6  A_contradicts_B  B_contradicts_A   \n",
       "4                3.4      A_neutral_B      B_neutral_A   \n",
       "\n",
       "                                 sentence_A_original  \\\n",
       "0  A group of children playing in a yard, a man i...   \n",
       "1  A group of children playing in a yard, a man i...   \n",
       "2  The children are playing outdoors, while a man...   \n",
       "3  The children are playing outdoors, while a man...   \n",
       "4  A group of children playing in a yard, a man i...   \n",
       "\n",
       "                                 sentence_B_original sentence_A_dataset  \\\n",
       "0  A group of children playing in a yard, a man i...             FLICKR   \n",
       "1  A group of children playing in a yard, a man i...             FLICKR   \n",
       "2  The children are playing outdoors, while a man...             FLICKR   \n",
       "3  The children are playing outdoors, while a man...             FLICKR   \n",
       "4  The children are playing outdoors, while a man...             FLICKR   \n",
       "\n",
       "  sentence_B_dataset SemEval_set  \n",
       "0             FLICKR       TRAIN  \n",
       "1             FLICKR       TRAIN  \n",
       "2             FLICKR       TRAIN  \n",
       "3             FLICKR       TRIAL  \n",
       "4             FLICKR       TRAIN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sentences = data.shape[0]\n",
    "num_labels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the Premise and Hypothesis along with the Entailment Label to construct our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['sentence_A','sentence_B']]\n",
    "Y = data['entailment_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process our label encoding now, but our sentences are not in a format which can be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder().fit(Y)\n",
    "Y = np_utils.to_categorical(le.transform(Y))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence_A holds the Premise while sentence_B holds the Hypothesis data.\n",
    "\n",
    "To tokenize our corpus we will need to stack these two columns into a single series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked = pd.concat([X['sentence_A'], X['sentence_B']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert question corpus into sequential encoding for LSTM\n",
    "vocab_size = 512\n",
    "sequence_length = 16\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(stacked)\n",
    "sequences = tokenizer.texts_to_sequences(stacked)\n",
    "x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "premise_sequences = x_text[:data.shape[0]]\n",
    "hypothesis_sequences = x_text[data.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "dense_units = 128\n",
    "dropout_rate = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, 128)\n",
    "premise_inputs = Input(shape=(sequence_length,))\n",
    "premise_embedding = embedding_layer(premise_inputs)\n",
    "premise_outputs, state_h, state_c = LSTM(128, return_state=True)(premise_embedding)\n",
    "\n",
    "hypothesis_inputs = Input(shape=(sequence_length,))\n",
    "hypothesis_embedding = embedding_layer(hypothesis_inputs)\n",
    "hypothesis_outputs = LSTM(128)(hypothesis_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "dense = Dense(dense_units, activation='relu', kernel_initializer=keras.initializers.he_normal(seed=None))(hypothesis_outputs)\n",
    "bn = BatchNormalization()(dense)\n",
    "dropout = Dropout(dropout_rate)(bn)\n",
    "outputs = Dense(num_labels, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 16, 128)      65536       input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, 128), (None, 131584      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 128)          131584      embedding_8[1][0]                \n",
      "                                                                 lstm_15[0][1]                    \n",
      "                                                                 lstm_15[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          16512       lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128)          512         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 346,115\n",
      "Trainable params: 345,859\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 7872 samples, validate on 1968 samples\n",
      "Epoch 1/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.9175 - acc: 0.5279 - val_loss: 1.8150 - val_acc: 0.2840\n",
      "Epoch 2/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.7802 - acc: 0.5926 - val_loss: 0.7353 - val_acc: 0.6575\n",
      "Epoch 3/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.7467 - acc: 0.6174 - val_loss: 0.9972 - val_acc: 0.4405\n",
      "Epoch 4/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.7275 - acc: 0.6334 - val_loss: 0.9184 - val_acc: 0.6575\n",
      "Epoch 5/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.6993 - acc: 0.6449 - val_loss: 0.7774 - val_acc: 0.6428\n",
      "Epoch 6/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.6918 - acc: 0.6519 - val_loss: 0.7962 - val_acc: 0.6468\n",
      "Epoch 7/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.6804 - acc: 0.6618 - val_loss: 0.8211 - val_acc: 0.6504\n",
      "Epoch 8/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.6682 - acc: 0.6719 - val_loss: 0.9509 - val_acc: 0.5401\n",
      "Epoch 9/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.6596 - acc: 0.6724 - val_loss: 0.9594 - val_acc: 0.4685\n",
      "Epoch 10/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.6452 - acc: 0.6818 - val_loss: 0.7939 - val_acc: 0.6286\n",
      "Epoch 11/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.6375 - acc: 0.6852 - val_loss: 0.8183 - val_acc: 0.5833\n",
      "Epoch 12/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.6223 - acc: 0.6951 - val_loss: 0.8619 - val_acc: 0.6103\n",
      "Epoch 13/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.6183 - acc: 0.7011 - val_loss: 0.8586 - val_acc: 0.5661\n",
      "Epoch 14/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.6136 - acc: 0.7019 - val_loss: 0.8401 - val_acc: 0.6443\n",
      "Epoch 15/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.5951 - acc: 0.7168 - val_loss: 0.8586 - val_acc: 0.5777\n",
      "Epoch 16/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.5922 - acc: 0.7182 - val_loss: 0.8707 - val_acc: 0.6092\n",
      "Epoch 17/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.5720 - acc: 0.7285 - val_loss: 1.0016 - val_acc: 0.5650\n",
      "Epoch 18/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.5619 - acc: 0.7322 - val_loss: 1.0934 - val_acc: 0.5127\n",
      "Epoch 19/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.5532 - acc: 0.7428 - val_loss: 0.9892 - val_acc: 0.5320\n",
      "Epoch 20/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.5381 - acc: 0.7524 - val_loss: 0.8874 - val_acc: 0.6133\n",
      "Epoch 21/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.5170 - acc: 0.7560 - val_loss: 1.0374 - val_acc: 0.5498\n",
      "Epoch 22/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.5100 - acc: 0.7651 - val_loss: 1.0160 - val_acc: 0.5864\n",
      "Epoch 23/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.4754 - acc: 0.7779 - val_loss: 1.0220 - val_acc: 0.5996\n",
      "Epoch 24/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.4529 - acc: 0.7945 - val_loss: 1.0421 - val_acc: 0.5666\n",
      "Epoch 25/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.4396 - acc: 0.8035 - val_loss: 1.0821 - val_acc: 0.6087\n",
      "Epoch 26/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.4148 - acc: 0.8244 - val_loss: 1.1933 - val_acc: 0.5869\n",
      "Epoch 27/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.4005 - acc: 0.8251 - val_loss: 1.1988 - val_acc: 0.5884\n",
      "Epoch 28/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.3747 - acc: 0.8371 - val_loss: 1.2654 - val_acc: 0.5854\n",
      "Epoch 29/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.3608 - acc: 0.8426 - val_loss: 1.3221 - val_acc: 0.5650\n",
      "Epoch 30/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.3404 - acc: 0.8622 - val_loss: 1.4208 - val_acc: 0.5493\n",
      "Epoch 31/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.3385 - acc: 0.8552 - val_loss: 1.2044 - val_acc: 0.6270\n",
      "Epoch 32/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.3101 - acc: 0.8745 - val_loss: 1.3709 - val_acc: 0.5935\n",
      "Epoch 33/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2989 - acc: 0.8753 - val_loss: 1.3682 - val_acc: 0.5854\n",
      "Epoch 34/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2907 - acc: 0.8857 - val_loss: 1.4166 - val_acc: 0.5955\n",
      "Epoch 35/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2643 - acc: 0.8961 - val_loss: 1.4296 - val_acc: 0.6133\n",
      "Epoch 36/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2613 - acc: 0.8956 - val_loss: 1.5522 - val_acc: 0.5986\n",
      "Epoch 37/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2358 - acc: 0.9096 - val_loss: 1.5409 - val_acc: 0.6138\n",
      "Epoch 38/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2489 - acc: 0.9047 - val_loss: 1.5864 - val_acc: 0.5945\n",
      "Epoch 39/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2438 - acc: 0.9045 - val_loss: 1.7831 - val_acc: 0.5783\n",
      "Epoch 40/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2273 - acc: 0.9145 - val_loss: 1.5126 - val_acc: 0.5965\n",
      "Epoch 41/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2157 - acc: 0.9182 - val_loss: 1.6451 - val_acc: 0.6138\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.2138 - acc: 0.9223 - val_loss: 1.5079 - val_acc: 0.6214\n",
      "Epoch 43/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.2006 - acc: 0.9215 - val_loss: 1.8448 - val_acc: 0.5955\n",
      "Epoch 44/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.1985 - acc: 0.9243 - val_loss: 1.8295 - val_acc: 0.5940\n",
      "Epoch 45/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1830 - acc: 0.9320 - val_loss: 1.8927 - val_acc: 0.6001\n",
      "Epoch 46/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1839 - acc: 0.9299 - val_loss: 1.6528 - val_acc: 0.6057\n",
      "Epoch 47/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.1871 - acc: 0.9313 - val_loss: 1.7215 - val_acc: 0.6159\n",
      "Epoch 48/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.1725 - acc: 0.9352 - val_loss: 1.7678 - val_acc: 0.6189\n",
      "Epoch 49/100\n",
      "7872/7872 [==============================] - 8s 995us/step - loss: 0.1523 - acc: 0.9447 - val_loss: 2.0416 - val_acc: 0.6128\n",
      "Epoch 50/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.1659 - acc: 0.9419 - val_loss: 1.7608 - val_acc: 0.6240\n",
      "Epoch 51/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.1598 - acc: 0.9403 - val_loss: 2.0438 - val_acc: 0.6108\n",
      "Epoch 52/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1536 - acc: 0.9441 - val_loss: 2.1217 - val_acc: 0.5991\n",
      "Epoch 53/100\n",
      "7872/7872 [==============================] - 13s 2ms/step - loss: 0.1462 - acc: 0.9446 - val_loss: 2.0311 - val_acc: 0.6072\n",
      "Epoch 54/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1459 - acc: 0.9479 - val_loss: 2.1106 - val_acc: 0.5960\n",
      "Epoch 55/100\n",
      "7872/7872 [==============================] - 14s 2ms/step - loss: 0.1474 - acc: 0.9445 - val_loss: 2.1392 - val_acc: 0.6082\n",
      "Epoch 56/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1471 - acc: 0.9442 - val_loss: 2.0489 - val_acc: 0.6133\n",
      "Epoch 57/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.1313 - acc: 0.9520 - val_loss: 2.0496 - val_acc: 0.6204\n",
      "Epoch 58/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.1387 - acc: 0.9486 - val_loss: 2.0396 - val_acc: 0.6118\n",
      "Epoch 59/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1363 - acc: 0.9493 - val_loss: 2.0474 - val_acc: 0.6143\n",
      "Epoch 60/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1196 - acc: 0.9591 - val_loss: 2.1377 - val_acc: 0.6194\n",
      "Epoch 61/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1182 - acc: 0.9573 - val_loss: 2.2262 - val_acc: 0.6128\n",
      "Epoch 62/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.1203 - acc: 0.9576 - val_loss: 2.1446 - val_acc: 0.6123\n",
      "Epoch 63/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.1120 - acc: 0.9591 - val_loss: 2.3885 - val_acc: 0.5920\n",
      "Epoch 64/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1272 - acc: 0.9530 - val_loss: 2.2370 - val_acc: 0.6016\n",
      "Epoch 65/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1130 - acc: 0.9605 - val_loss: 2.2388 - val_acc: 0.6265\n",
      "Epoch 66/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1086 - acc: 0.9593 - val_loss: 2.2202 - val_acc: 0.6286\n",
      "Epoch 67/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1032 - acc: 0.9618 - val_loss: 2.5777 - val_acc: 0.5930\n",
      "Epoch 68/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.1051 - acc: 0.9629 - val_loss: 2.4529 - val_acc: 0.6235\n",
      "Epoch 69/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.1062 - acc: 0.9616 - val_loss: 2.2653 - val_acc: 0.6220\n",
      "Epoch 70/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0881 - acc: 0.9689 - val_loss: 2.6767 - val_acc: 0.6011\n",
      "Epoch 71/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0833 - acc: 0.9710 - val_loss: 2.4360 - val_acc: 0.6159\n",
      "Epoch 72/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0842 - acc: 0.9705 - val_loss: 2.4652 - val_acc: 0.6169\n",
      "Epoch 73/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0999 - acc: 0.9628 - val_loss: 2.4646 - val_acc: 0.6021\n",
      "Epoch 74/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.0955 - acc: 0.9661 - val_loss: 2.3895 - val_acc: 0.6214\n",
      "Epoch 75/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0984 - acc: 0.9629 - val_loss: 2.3885 - val_acc: 0.6113\n",
      "Epoch 76/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0921 - acc: 0.9665 - val_loss: 2.8998 - val_acc: 0.5879\n",
      "Epoch 77/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0878 - acc: 0.9686 - val_loss: 2.6314 - val_acc: 0.6037\n",
      "Epoch 78/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0902 - acc: 0.9682 - val_loss: 2.6590 - val_acc: 0.60470s - loss: 0.0901 - acc:\n",
      "Epoch 79/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0773 - acc: 0.9718 - val_loss: 2.5229 - val_acc: 0.6245\n",
      "Epoch 80/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0828 - acc: 0.9701 - val_loss: 2.3841 - val_acc: 0.6164\n",
      "Epoch 81/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.1013 - acc: 0.9639 - val_loss: 2.6129 - val_acc: 0.6006\n",
      "Epoch 82/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.0855 - acc: 0.9676 - val_loss: 2.3956 - val_acc: 0.6230\n",
      "Epoch 83/100\n",
      "7872/7872 [==============================] - 12s 2ms/step - loss: 0.0807 - acc: 0.9707 - val_loss: 2.6929 - val_acc: 0.6092\n",
      "Epoch 84/100\n",
      "7872/7872 [==============================] - 11s 1ms/step - loss: 0.0630 - acc: 0.9756 - val_loss: 3.0914 - val_acc: 0.5976\n",
      "Epoch 85/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0755 - acc: 0.9718 - val_loss: 2.5227 - val_acc: 0.6153\n",
      "Epoch 86/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0712 - acc: 0.9743 - val_loss: 2.6003 - val_acc: 0.6103\n",
      "Epoch 87/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0569 - acc: 0.9792 - val_loss: 2.9154 - val_acc: 0.6057\n",
      "Epoch 88/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0621 - acc: 0.9784 - val_loss: 2.6981 - val_acc: 0.6159\n",
      "Epoch 89/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0772 - acc: 0.9723 - val_loss: 2.5464 - val_acc: 0.6148\n",
      "Epoch 90/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0821 - acc: 0.9695 - val_loss: 2.5385 - val_acc: 0.6209\n",
      "Epoch 91/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0810 - acc: 0.9704 - val_loss: 2.6068 - val_acc: 0.6220\n",
      "Epoch 92/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.0771 - acc: 0.9723 - val_loss: 2.5338 - val_acc: 0.6169\n",
      "Epoch 93/100\n",
      "7872/7872 [==============================] - 8s 1ms/step - loss: 0.0740 - acc: 0.9722 - val_loss: 2.4650 - val_acc: 0.6220\n",
      "Epoch 94/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0694 - acc: 0.9751 - val_loss: 2.6307 - val_acc: 0.6270\n",
      "Epoch 95/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0680 - acc: 0.9742 - val_loss: 2.5572 - val_acc: 0.6143\n",
      "Epoch 96/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0689 - acc: 0.9759 - val_loss: 2.4769 - val_acc: 0.6347\n",
      "Epoch 97/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0637 - acc: 0.9766 - val_loss: 2.7932 - val_acc: 0.6016\n",
      "Epoch 98/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0583 - acc: 0.9784 - val_loss: 2.6729 - val_acc: 0.6128\n",
      "Epoch 99/100\n",
      "7872/7872 [==============================] - 9s 1ms/step - loss: 0.0497 - acc: 0.9806 - val_loss: 2.8803 - val_acc: 0.6092\n",
      "Epoch 100/100\n",
      "7872/7872 [==============================] - 10s 1ms/step - loss: 0.0646 - acc: 0.9766 - val_loss: 2.6579 - val_acc: 0.6235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120be77d0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[premise_inputs, hypothesis_inputs], outputs=outputs)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([premise_sequences, hypothesis_sequences], Y, batch_size=batch_size, epochs=epochs, shuffle=True, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
