{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import learning_curve\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../Datasets/SICK/SICK.txt'\n",
    "data = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>entailment_label</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_AB</th>\n",
       "      <th>entailment_BA</th>\n",
       "      <th>sentence_A_original</th>\n",
       "      <th>sentence_B_original</th>\n",
       "      <th>sentence_A_dataset</th>\n",
       "      <th>sentence_B_dataset</th>\n",
       "      <th>SemEval_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.7</td>\n",
       "      <td>A_entails_B</td>\n",
       "      <td>B_entails_A</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>3.6</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_contradicts_A</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.4</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children playing in a yard, a man i...</td>\n",
       "      <td>The children are playing outdoors, while a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  A group of kids is playing in a yard and an ol...   \n",
       "1        2  A group of children is playing in the house an...   \n",
       "2        3  The young boys are playing outdoors and the ma...   \n",
       "3        4  The young boys are playing outdoors and the ma...   \n",
       "4        5  The kids are playing outdoors near a man with ...   \n",
       "\n",
       "                                          sentence_B entailment_label  \\\n",
       "0  A group of boys in a yard is playing and a man...          NEUTRAL   \n",
       "1  A group of kids is playing in a yard and an ol...          NEUTRAL   \n",
       "2  The kids are playing outdoors near a man with ...       ENTAILMENT   \n",
       "3  There is no boy playing outdoors and there is ...    CONTRADICTION   \n",
       "4  A group of kids is playing in a yard and an ol...          NEUTRAL   \n",
       "\n",
       "   relatedness_score    entailment_AB    entailment_BA  \\\n",
       "0                4.5      A_neutral_B      B_neutral_A   \n",
       "1                3.2  A_contradicts_B      B_neutral_A   \n",
       "2                4.7      A_entails_B      B_entails_A   \n",
       "3                3.6  A_contradicts_B  B_contradicts_A   \n",
       "4                3.4      A_neutral_B      B_neutral_A   \n",
       "\n",
       "                                 sentence_A_original  \\\n",
       "0  A group of children playing in a yard, a man i...   \n",
       "1  A group of children playing in a yard, a man i...   \n",
       "2  The children are playing outdoors, while a man...   \n",
       "3  The children are playing outdoors, while a man...   \n",
       "4  A group of children playing in a yard, a man i...   \n",
       "\n",
       "                                 sentence_B_original sentence_A_dataset  \\\n",
       "0  A group of children playing in a yard, a man i...             FLICKR   \n",
       "1  A group of children playing in a yard, a man i...             FLICKR   \n",
       "2  The children are playing outdoors, while a man...             FLICKR   \n",
       "3  The children are playing outdoors, while a man...             FLICKR   \n",
       "4  The children are playing outdoors, while a man...             FLICKR   \n",
       "\n",
       "  sentence_B_dataset SemEval_set  \n",
       "0             FLICKR       TRAIN  \n",
       "1             FLICKR       TRAIN  \n",
       "2             FLICKR       TRAIN  \n",
       "3             FLICKR       TRIAL  \n",
       "4             FLICKR       TRAIN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = data.shape[0]\n",
    "num_labels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the Premise and Hypothesis along with the Entailment Label to construct our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['sentence_A','sentence_B']]\n",
    "Y = data['entailment_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process our label encoding now, but our sentences are not in a format which can be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder().fit(Y)\n",
    "Y = np_utils.to_categorical(le.transform(Y))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence_A holds the Premise while sentence_B holds the Hypothesis data.\n",
    "\n",
    "To tokenize our corpus we will need to stack these two columns into a single series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = pd.concat([X['sentence_A'], X['sentence_B']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert question corpus into sequential encoding for LSTM\n",
    "vocab_size = 512\n",
    "sequence_length = 16\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(stacked)\n",
    "sequences = tokenizer.texts_to_sequences(stacked)\n",
    "x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "premise_sequences = x_text[:data.shape[0]]\n",
    "hypothesis_sequences = x_text[data.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "dense_units = 128\n",
    "dropout_rate = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, 128)\n",
    "premise_inputs = Input(shape=(sequence_length,))\n",
    "premise_embedding = embedding_layer(premise_inputs)\n",
    "premise_outputs, state_h, state_c = LSTM(128, return_state=True)(premise_embedding)\n",
    "\n",
    "hypothesis_inputs = Input(shape=(sequence_length,))\n",
    "hypothesis_embedding = embedding_layer(hypothesis_inputs)\n",
    "hypothesis_outputs = LSTM(128)(hypothesis_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "dense = Dense(dense_units, activation='relu', kernel_initializer=keras.initializers.he_normal(seed=None))(hypothesis_outputs)\n",
    "bn = BatchNormalization()(dense)\n",
    "dropout = Dropout(dropout_rate)(bn)\n",
    "outputs = Dense(num_labels, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 16, 128)      65536       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          131584      embedding_1[1][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 346,115\n",
      "Trainable params: 345,859\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tzhang270/Desktop/Projects/ML/Models/Attention-LSTM/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7872 samples, validate on 1968 samples\n",
      "Epoch 1/100\n",
      "7872/7872 [==============================] - 7s 951us/step - loss: 0.9303 - accuracy: 0.5201 - val_loss: 0.7986 - val_accuracy: 0.6753\n",
      "Epoch 2/100\n",
      "7872/7872 [==============================] - 6s 721us/step - loss: 0.7747 - accuracy: 0.6071 - val_loss: 0.7665 - val_accuracy: 0.6753\n",
      "Epoch 3/100\n",
      "7872/7872 [==============================] - 6s 706us/step - loss: 0.7389 - accuracy: 0.6184 - val_loss: 0.7721 - val_accuracy: 0.6753\n",
      "Epoch 4/100\n",
      "7872/7872 [==============================] - 6s 726us/step - loss: 0.7147 - accuracy: 0.6353 - val_loss: 0.7267 - val_accuracy: 0.6728\n",
      "Epoch 5/100\n",
      "7872/7872 [==============================] - 6s 780us/step - loss: 0.6885 - accuracy: 0.6533 - val_loss: 0.7408 - val_accuracy: 0.6453\n",
      "Epoch 6/100\n",
      "7872/7872 [==============================] - 7s 862us/step - loss: 0.6815 - accuracy: 0.6535 - val_loss: 0.7760 - val_accuracy: 0.6103\n",
      "Epoch 7/100\n",
      "7872/7872 [==============================] - 6s 824us/step - loss: 0.6706 - accuracy: 0.6686 - val_loss: 0.8012 - val_accuracy: 0.6387\n",
      "Epoch 8/100\n",
      "7872/7872 [==============================] - 7s 873us/step - loss: 0.6564 - accuracy: 0.6762 - val_loss: 0.8052 - val_accuracy: 0.6418\n",
      "Epoch 9/100\n",
      "7872/7872 [==============================] - 6s 797us/step - loss: 0.6431 - accuracy: 0.6847 - val_loss: 0.8509 - val_accuracy: 0.5813\n",
      "Epoch 10/100\n",
      "7872/7872 [==============================] - 6s 768us/step - loss: 0.6342 - accuracy: 0.6878 - val_loss: 0.8085 - val_accuracy: 0.6260\n",
      "Epoch 11/100\n",
      "7872/7872 [==============================] - 6s 764us/step - loss: 0.6234 - accuracy: 0.6965 - val_loss: 0.8312 - val_accuracy: 0.6118\n",
      "Epoch 12/100\n",
      "7872/7872 [==============================] - 6s 772us/step - loss: 0.6157 - accuracy: 0.7057 - val_loss: 0.8258 - val_accuracy: 0.6280\n",
      "Epoch 13/100\n",
      "7872/7872 [==============================] - 6s 767us/step - loss: 0.6003 - accuracy: 0.7074 - val_loss: 0.9410 - val_accuracy: 0.5920\n",
      "Epoch 14/100\n",
      "7872/7872 [==============================] - 5s 698us/step - loss: 0.5949 - accuracy: 0.7107 - val_loss: 1.1373 - val_accuracy: 0.4761\n",
      "Epoch 15/100\n",
      "7872/7872 [==============================] - 6s 775us/step - loss: 0.5818 - accuracy: 0.7190 - val_loss: 1.0040 - val_accuracy: 0.5015\n",
      "Epoch 16/100\n",
      "7872/7872 [==============================] - 6s 719us/step - loss: 0.5814 - accuracy: 0.7224 - val_loss: 0.8603 - val_accuracy: 0.5930\n",
      "Epoch 17/100\n",
      "7872/7872 [==============================] - 5s 692us/step - loss: 0.5502 - accuracy: 0.7384 - val_loss: 0.8978 - val_accuracy: 0.6311\n",
      "Epoch 18/100\n",
      "7872/7872 [==============================] - 6s 704us/step - loss: 0.5271 - accuracy: 0.7536 - val_loss: 0.9647 - val_accuracy: 0.6225\n",
      "Epoch 19/100\n",
      "7872/7872 [==============================] - 6s 732us/step - loss: 0.5171 - accuracy: 0.7595 - val_loss: 0.9603 - val_accuracy: 0.5666\n",
      "Epoch 20/100\n",
      "7872/7872 [==============================] - 6s 816us/step - loss: 0.4734 - accuracy: 0.7896 - val_loss: 0.9197 - val_accuracy: 0.6118\n",
      "Epoch 21/100\n",
      "7872/7872 [==============================] - 6s 730us/step - loss: 0.4495 - accuracy: 0.7999 - val_loss: 0.9948 - val_accuracy: 0.6128\n",
      "Epoch 22/100\n",
      "7872/7872 [==============================] - 6s 726us/step - loss: 0.4174 - accuracy: 0.8195 - val_loss: 1.0275 - val_accuracy: 0.6148\n",
      "Epoch 23/100\n",
      "7872/7872 [==============================] - 6s 733us/step - loss: 0.3957 - accuracy: 0.8328 - val_loss: 1.0278 - val_accuracy: 0.6164\n",
      "Epoch 24/100\n",
      "7872/7872 [==============================] - 6s 790us/step - loss: 0.3798 - accuracy: 0.8430 - val_loss: 1.1521 - val_accuracy: 0.5803\n",
      "Epoch 25/100\n",
      "7872/7872 [==============================] - 6s 796us/step - loss: 0.3635 - accuracy: 0.8496 - val_loss: 1.1173 - val_accuracy: 0.6184\n",
      "Epoch 26/100\n",
      "7872/7872 [==============================] - 6s 775us/step - loss: 0.3367 - accuracy: 0.8636 - val_loss: 1.2194 - val_accuracy: 0.5965\n",
      "Epoch 27/100\n",
      "7872/7872 [==============================] - 6s 746us/step - loss: 0.3205 - accuracy: 0.8676 - val_loss: 1.2550 - val_accuracy: 0.5996\n",
      "Epoch 28/100\n",
      "7872/7872 [==============================] - 6s 768us/step - loss: 0.3090 - accuracy: 0.8749 - val_loss: 1.3158 - val_accuracy: 0.5818\n",
      "Epoch 29/100\n",
      "7872/7872 [==============================] - 6s 781us/step - loss: 0.2908 - accuracy: 0.8800 - val_loss: 1.4148 - val_accuracy: 0.5762\n",
      "Epoch 30/100\n",
      "7872/7872 [==============================] - 6s 752us/step - loss: 0.2815 - accuracy: 0.8910 - val_loss: 1.3984 - val_accuracy: 0.5859\n",
      "Epoch 31/100\n",
      "7872/7872 [==============================] - 6s 775us/step - loss: 0.2621 - accuracy: 0.8977 - val_loss: 1.3443 - val_accuracy: 0.6235\n",
      "Epoch 32/100\n",
      "7872/7872 [==============================] - 6s 776us/step - loss: 0.2551 - accuracy: 0.8981 - val_loss: 1.4630 - val_accuracy: 0.6032\n",
      "Epoch 33/100\n",
      "7872/7872 [==============================] - 6s 764us/step - loss: 0.2461 - accuracy: 0.9018 - val_loss: 1.4259 - val_accuracy: 0.6169\n",
      "Epoch 34/100\n",
      "7872/7872 [==============================] - 6s 775us/step - loss: 0.2406 - accuracy: 0.9071 - val_loss: 1.6771 - val_accuracy: 0.5945\n",
      "Epoch 35/100\n",
      "7872/7872 [==============================] - 6s 748us/step - loss: 0.2245 - accuracy: 0.9131 - val_loss: 1.5921 - val_accuracy: 0.6011\n",
      "Epoch 36/100\n",
      "7872/7872 [==============================] - 6s 753us/step - loss: 0.2220 - accuracy: 0.9154 - val_loss: 1.5156 - val_accuracy: 0.6087\n",
      "Epoch 37/100\n",
      "7872/7872 [==============================] - 6s 731us/step - loss: 0.1985 - accuracy: 0.9247 - val_loss: 1.6187 - val_accuracy: 0.6179\n",
      "Epoch 38/100\n",
      "7872/7872 [==============================] - 6s 812us/step - loss: 0.1999 - accuracy: 0.9256 - val_loss: 1.7860 - val_accuracy: 0.5899\n",
      "Epoch 39/100\n",
      "7872/7872 [==============================] - 6s 727us/step - loss: 0.1942 - accuracy: 0.9262 - val_loss: 1.7803 - val_accuracy: 0.6077\n",
      "Epoch 40/100\n",
      "7872/7872 [==============================] - 6s 717us/step - loss: 0.1892 - accuracy: 0.9263 - val_loss: 1.6551 - val_accuracy: 0.6077\n",
      "Epoch 41/100\n",
      "7872/7872 [==============================] - 6s 795us/step - loss: 0.1769 - accuracy: 0.9342 - val_loss: 1.7124 - val_accuracy: 0.6235\n",
      "Epoch 42/100\n",
      "7872/7872 [==============================] - 6s 734us/step - loss: 0.1724 - accuracy: 0.9341 - val_loss: 1.8702 - val_accuracy: 0.6067\n",
      "Epoch 43/100\n",
      "7872/7872 [==============================] - 6s 807us/step - loss: 0.1600 - accuracy: 0.9385 - val_loss: 2.3077 - val_accuracy: 0.5828\n",
      "Epoch 44/100\n",
      "7872/7872 [==============================] - 6s 730us/step - loss: 0.1670 - accuracy: 0.9381 - val_loss: 1.8421 - val_accuracy: 0.6143\n",
      "Epoch 45/100\n",
      "7872/7872 [==============================] - 6s 708us/step - loss: 0.1623 - accuracy: 0.9411 - val_loss: 1.9285 - val_accuracy: 0.6118\n",
      "Epoch 46/100\n",
      "7872/7872 [==============================] - 6s 703us/step - loss: 0.1477 - accuracy: 0.9460 - val_loss: 1.9916 - val_accuracy: 0.6092\n",
      "Epoch 47/100\n",
      "7872/7872 [==============================] - 6s 738us/step - loss: 0.1417 - accuracy: 0.9475 - val_loss: 1.9634 - val_accuracy: 0.6143\n",
      "Epoch 48/100\n",
      "7872/7872 [==============================] - 6s 790us/step - loss: 0.1344 - accuracy: 0.9498 - val_loss: 1.9092 - val_accuracy: 0.6352\n",
      "Epoch 49/100\n",
      "7872/7872 [==============================] - 6s 735us/step - loss: 0.1474 - accuracy: 0.9466 - val_loss: 1.8729 - val_accuracy: 0.6331\n",
      "Epoch 50/100\n",
      "7872/7872 [==============================] - 5s 686us/step - loss: 0.1482 - accuracy: 0.9463 - val_loss: 2.1182 - val_accuracy: 0.6037\n",
      "Epoch 51/100\n",
      "7872/7872 [==============================] - 6s 701us/step - loss: 0.1274 - accuracy: 0.9530 - val_loss: 2.0966 - val_accuracy: 0.5889\n",
      "Epoch 52/100\n",
      "7872/7872 [==============================] - 6s 715us/step - loss: 0.1366 - accuracy: 0.9474 - val_loss: 2.1488 - val_accuracy: 0.6240\n",
      "Epoch 53/100\n",
      "7872/7872 [==============================] - 6s 705us/step - loss: 0.1342 - accuracy: 0.9524 - val_loss: 2.0221 - val_accuracy: 0.6174\n",
      "Epoch 54/100\n",
      "7872/7872 [==============================] - 6s 712us/step - loss: 0.1179 - accuracy: 0.9567 - val_loss: 2.1215 - val_accuracy: 0.6108\n",
      "Epoch 55/100\n",
      "7872/7872 [==============================] - 6s 700us/step - loss: 0.1058 - accuracy: 0.9625 - val_loss: 2.0907 - val_accuracy: 0.6214\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7872/7872 [==============================] - 6s 708us/step - loss: 0.1089 - accuracy: 0.9605 - val_loss: 2.1713 - val_accuracy: 0.6169\n",
      "Epoch 57/100\n",
      "7872/7872 [==============================] - 6s 712us/step - loss: 0.1094 - accuracy: 0.9609 - val_loss: 2.3805 - val_accuracy: 0.6067\n",
      "Epoch 58/100\n",
      "7872/7872 [==============================] - 6s 718us/step - loss: 0.1227 - accuracy: 0.9564 - val_loss: 2.0858 - val_accuracy: 0.6194\n",
      "Epoch 59/100\n",
      "7872/7872 [==============================] - 6s 727us/step - loss: 0.1159 - accuracy: 0.9562 - val_loss: 2.2386 - val_accuracy: 0.6062\n",
      "Epoch 60/100\n",
      "7872/7872 [==============================] - 6s 699us/step - loss: 0.1046 - accuracy: 0.9620 - val_loss: 2.3378 - val_accuracy: 0.5935\n",
      "Epoch 61/100\n",
      "7872/7872 [==============================] - 6s 699us/step - loss: 0.1168 - accuracy: 0.9546 - val_loss: 1.9805 - val_accuracy: 0.6321\n",
      "Epoch 62/100\n",
      "7872/7872 [==============================] - 6s 700us/step - loss: 0.1085 - accuracy: 0.9620 - val_loss: 2.1770 - val_accuracy: 0.6092\n",
      "Epoch 63/100\n",
      "7872/7872 [==============================] - 6s 699us/step - loss: 0.1082 - accuracy: 0.9629 - val_loss: 2.3099 - val_accuracy: 0.5879\n",
      "Epoch 64/100\n",
      "7872/7872 [==============================] - 6s 707us/step - loss: 0.0917 - accuracy: 0.9663 - val_loss: 2.4659 - val_accuracy: 0.5965\n",
      "Epoch 65/100\n",
      "7872/7872 [==============================] - 6s 760us/step - loss: 0.0889 - accuracy: 0.9651 - val_loss: 2.5366 - val_accuracy: 0.5849\n",
      "Epoch 66/100\n",
      "7872/7872 [==============================] - 6s 804us/step - loss: 0.0972 - accuracy: 0.9676 - val_loss: 2.1041 - val_accuracy: 0.6341\n",
      "Epoch 67/100\n",
      "7872/7872 [==============================] - 5s 688us/step - loss: 0.0935 - accuracy: 0.9658 - val_loss: 2.1834 - val_accuracy: 0.6280\n",
      "Epoch 68/100\n",
      "7872/7872 [==============================] - 5s 679us/step - loss: 0.0901 - accuracy: 0.9668 - val_loss: 2.2107 - val_accuracy: 0.6291\n",
      "Epoch 69/100\n",
      "7872/7872 [==============================] - 5s 676us/step - loss: 0.0803 - accuracy: 0.9705 - val_loss: 2.3072 - val_accuracy: 0.6326\n",
      "Epoch 70/100\n",
      "7872/7872 [==============================] - 5s 676us/step - loss: 0.0852 - accuracy: 0.9698 - val_loss: 2.3029 - val_accuracy: 0.6250\n",
      "Epoch 71/100\n",
      "7872/7872 [==============================] - 5s 673us/step - loss: 0.0799 - accuracy: 0.9736 - val_loss: 2.5321 - val_accuracy: 0.6204\n",
      "Epoch 72/100\n",
      "7872/7872 [==============================] - 5s 679us/step - loss: 0.0736 - accuracy: 0.9729 - val_loss: 2.6306 - val_accuracy: 0.6026\n",
      "Epoch 73/100\n",
      "7872/7872 [==============================] - 5s 690us/step - loss: 0.0813 - accuracy: 0.9701 - val_loss: 2.6230 - val_accuracy: 0.5859\n",
      "Epoch 74/100\n",
      "7872/7872 [==============================] - 6s 774us/step - loss: 0.0932 - accuracy: 0.9661 - val_loss: 2.2929 - val_accuracy: 0.6199\n",
      "Epoch 75/100\n",
      "7872/7872 [==============================] - 6s 806us/step - loss: 0.0937 - accuracy: 0.9643 - val_loss: 2.1951 - val_accuracy: 0.6245\n",
      "Epoch 76/100\n",
      "7872/7872 [==============================] - 6s 741us/step - loss: 0.0803 - accuracy: 0.9714 - val_loss: 2.6622 - val_accuracy: 0.6001\n",
      "Epoch 77/100\n",
      "7872/7872 [==============================] - 6s 752us/step - loss: 0.0775 - accuracy: 0.9724 - val_loss: 2.4418 - val_accuracy: 0.6062\n",
      "Epoch 78/100\n",
      "7872/7872 [==============================] - 6s 723us/step - loss: 0.0718 - accuracy: 0.9737 - val_loss: 2.6994 - val_accuracy: 0.6133\n",
      "Epoch 79/100\n",
      "7872/7872 [==============================] - 5s 692us/step - loss: 0.0671 - accuracy: 0.9760 - val_loss: 2.4425 - val_accuracy: 0.6230\n",
      "Epoch 80/100\n",
      "7872/7872 [==============================] - 5s 687us/step - loss: 0.0697 - accuracy: 0.9745 - val_loss: 2.4362 - val_accuracy: 0.6341\n",
      "Epoch 81/100\n",
      "7872/7872 [==============================] - 6s 722us/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 2.4450 - val_accuracy: 0.6133\n",
      "Epoch 82/100\n",
      "7872/7872 [==============================] - 5s 674us/step - loss: 0.0613 - accuracy: 0.9770 - val_loss: 2.6045 - val_accuracy: 0.6286\n",
      "Epoch 83/100\n",
      "7872/7872 [==============================] - 5s 680us/step - loss: 0.0650 - accuracy: 0.9771 - val_loss: 2.6510 - val_accuracy: 0.6067\n",
      "Epoch 84/100\n",
      "7872/7872 [==============================] - 5s 680us/step - loss: 0.0770 - accuracy: 0.9713 - val_loss: 2.4247 - val_accuracy: 0.6250\n",
      "Epoch 85/100\n",
      "7872/7872 [==============================] - 5s 684us/step - loss: 0.0723 - accuracy: 0.9721 - val_loss: 2.5688 - val_accuracy: 0.6118\n",
      "Epoch 86/100\n",
      "7872/7872 [==============================] - 6s 712us/step - loss: 0.0868 - accuracy: 0.9693 - val_loss: 2.5685 - val_accuracy: 0.5960\n",
      "Epoch 87/100\n",
      "7872/7872 [==============================] - 6s 822us/step - loss: 0.0707 - accuracy: 0.9731 - val_loss: 2.4446 - val_accuracy: 0.6240\n",
      "Epoch 88/100\n",
      "7872/7872 [==============================] - 5s 668us/step - loss: 0.0532 - accuracy: 0.9807 - val_loss: 2.5800 - val_accuracy: 0.6220\n",
      "Epoch 89/100\n",
      "7872/7872 [==============================] - 5s 693us/step - loss: 0.0526 - accuracy: 0.9804 - val_loss: 2.8613 - val_accuracy: 0.6006\n",
      "Epoch 90/100\n",
      "7872/7872 [==============================] - 6s 716us/step - loss: 0.0603 - accuracy: 0.9784 - val_loss: 2.5280 - val_accuracy: 0.6230\n",
      "Epoch 91/100\n",
      "7872/7872 [==============================] - 6s 789us/step - loss: 0.0566 - accuracy: 0.9789 - val_loss: 2.7587 - val_accuracy: 0.6189\n",
      "Epoch 92/100\n",
      "7872/7872 [==============================] - 6s 718us/step - loss: 0.0541 - accuracy: 0.9812 - val_loss: 2.8006 - val_accuracy: 0.6214\n",
      "Epoch 93/100\n",
      "7872/7872 [==============================] - 5s 683us/step - loss: 0.0541 - accuracy: 0.9813 - val_loss: 2.6199 - val_accuracy: 0.6265\n",
      "Epoch 94/100\n",
      "7872/7872 [==============================] - 5s 667us/step - loss: 0.0583 - accuracy: 0.9769 - val_loss: 2.7069 - val_accuracy: 0.6204\n",
      "Epoch 95/100\n",
      "7872/7872 [==============================] - 5s 685us/step - loss: 0.0650 - accuracy: 0.9780 - val_loss: 2.6489 - val_accuracy: 0.6270\n",
      "Epoch 96/100\n",
      "7872/7872 [==============================] - 5s 687us/step - loss: 0.0615 - accuracy: 0.9778 - val_loss: 2.5610 - val_accuracy: 0.6260\n",
      "Epoch 97/100\n",
      "7872/7872 [==============================] - 6s 706us/step - loss: 0.0575 - accuracy: 0.9790 - val_loss: 2.7947 - val_accuracy: 0.6245\n",
      "Epoch 98/100\n",
      "7872/7872 [==============================] - 5s 691us/step - loss: 0.0529 - accuracy: 0.9807 - val_loss: 2.7182 - val_accuracy: 0.6311\n",
      "Epoch 99/100\n",
      "7872/7872 [==============================] - 5s 683us/step - loss: 0.0457 - accuracy: 0.9827 - val_loss: 2.8509 - val_accuracy: 0.6280\n",
      "Epoch 100/100\n",
      "7872/7872 [==============================] - 5s 691us/step - loss: 0.0459 - accuracy: 0.9808 - val_loss: 2.7717 - val_accuracy: 0.6220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x156cac2e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[premise_inputs, hypothesis_inputs], outputs=outputs)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([premise_sequences, hypothesis_sequences], Y, batch_size=batch_size, epochs=epochs, shuffle=True, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
